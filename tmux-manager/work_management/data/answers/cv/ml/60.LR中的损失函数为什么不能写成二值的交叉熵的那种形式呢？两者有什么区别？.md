## 怎样理解LR的损失函数？

* 损失函数？目标函数？似然函数？
  LR这个感觉叫目标函数或者是最大似然函数会更加的合理，但是叫目标函数也可以，如果是叫损失函数的话，我们一般想着的是最小化损失函数，这样会有点歧义，实际上。对于LR的话，我们是最大化这个似然函数。为什么叫做似然函数，因为这个目标函数是通过所有的样本的联合概率得到的。既然这些样本出现了或者说是存在了，按理来说，它们的联合概率应该是最大。

* 比较LR的损失函数和交叉熵的区别？
  $$
  LR\_loss = \sum_{i=1}^{n}(1-\hat y_{i})^{y_{i}}*\hat y_{i}^{1-y_{i}} \\
  或者使用log之后的损失函数为：\\
  LR\_loss = \sum_{i=1}^{n}y_{i}log(1-\hat y_{i}) + (1-y_{i})log\hat y_{i}
  $$
  

上面公式中的损失本来是用这个表示的$(1-\hat y_{i})^{y_{i}}*\hat y_{i}^{1-y_{i}}$, 这个公式的意思就是，当真实的标签为1的时候，损失是$1-\hat y_{i}$, 但真实的标签为0的时候，损失是$\hat y_{i}-0$, ==为什么要用这个表示呢？根本原因就是编码的原因，这个不同于one-hot的编码，one-hot的编码，对于有两个类别的来说，起码上是有两类的，但是这个只有一个输出，所以这里就体现了和二类的交叉熵不同的地方==。
		还有就是对于上面的单个损失，直接是进行去log，然后就是得到了一个叫做log损失的损失函数。 