#### 关于梯度的一些测试和理解

* 在pytorch中，梯度是作为tensor这个数据类型中的一个属性。可以使用ensor_object.grad来进行访问具体的梯度的情况。所以说，梯度就是蕴含在变量中的，比如说对于一个输入图像x，然后经过conv，得到y，然后y经过非线性变换得到z，x、y、z都是tensor，所以，它们都含有梯度的这个属性。是什么对它们的梯度呢？==是lossfunction！==

* 突然发现，pytorch在使用loss.backward() j进行反向传播的时候，并没有保留计算图中的中间节点的梯度？而是仅仅保留叶子节点的梯度；

```
UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at  C:\actions-runner\_work\pytorch\pytorch\builder\windows\pytorch\build\aten\src\ATen/core/TensorBody.h:482.)
  return self._grad
```

那么这样看来的话，在深度学习网络中，输入是一个x，进行正向传播，计算得到loss，然后loss进行反向传播的时候，我们只得到x这个变量的梯度，中间的那些节点的梯度都是没有被保留下来，这样做的目的就是为了节省内存空间啊。

如果你想保留中间节点的梯度，可以使用：middle_tensor_object.retain_grad()，这个函数，后面打印tensor的grad的话，就可以得到了。

```python
import torch
import torchvision

# the variant
x = torch.tensor(1.0, requires_grad=True)
y = 2 * x + 1
y.retain_grad()
z = y ** 2
z.retain_grad()
loss = 1 - z
loss.retain_grad()
print(z)
# before backward
print("before backward:")
print(f"x's gradient: {x.grad}")
print("----------------------------------------")
loss.backward()
print("after backward:")
print(f"x's gradient: {x.grad}")
print(f"y gradient: {y.grad}")
print(f"z gradient: {z.grad}")
print(f"loss gradient: {loss.grad}")
# the result
x's gradient: None
----------------------------------------
after backward:
x's gradient: -12.0
y gradient: -6.0
z gradient: -1.0
loss gradient: 1.0

Process finished with exit code 0
#
```

* 再理清一下深度学习的卷积的求梯度的过程，假设我们的图像的输入是x，然后就是经过一个卷积层（卷积操作和非线性激活函数），然后经过一个后面的后处理过程post，然后得到一个损失函数。看看这个过程中的梯度是怎样求取的，中间的变量是怎样，以及中间涉及到的梯度又是怎样的。
  $$
  x:input \\
  x_{c}: convolution\ operator \\
  x_{a}: activation\ function \\ 
  loss \\
  Then\ we\ can\ get\ the\ gradient\ of\ the\ x\\
  \frac{\partial loss}{\partial w} = \frac{\partial loss}{\partial x_{a}}*\frac{\partial x_{a}}{\partial x_{c}}*\frac{\partial x_{c}}{\partial w}
  $$

* 那些层有参数？

  卷积层，BN层，特别要注意，进行非线性的激活函数这个是没有参数的!
  
* 参数藏在哪里？

  参数，这个也是一个tensor，这个是藏在某一个层中的，作为某一个抽象层中的属性的。比如说，c1 = nn.conv2d(....), 然后就是c1就是一个卷积层的对象，这个c1中就包含了一个weights，这个weighs就是这个卷积层的参数。
  
* 所以一开始前面的关于梯度应该就是参数的梯度，而不是输入或者特征图的这些的梯度！ 所以这就错了！
  ==从上面的公式可以看出，计算浅层的权重的梯度，会用到深层的权重的值。==

* 推理一下BN层的反向传播的情况。
* 还是不是很理解累积梯度中的loss需要除以累积的迭代的次数。