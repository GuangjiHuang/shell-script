* DCGAN的原理
GAN就是生成对抗性网络的意思。这是一个网络，DC表示的是由直接的卷积层组成的网络。这个模型的组成的话，有两部分，一个是生成器，另一个是判别器。然后说一下它们的工作原理，生成器的输入就是一个随机数，输出一张我们希望得到的图像，这个图像我们希望它和我们的目标很像。然后判别器本质上就是一个分类器，我们把原来真实的数据的标签标为1，我们通过生成器生成的样本数据的标签标为0。假设我们已经提前训练了分类器，把我们的真实样本和生成的样本打好标签之后，给到分类器去进行分类，如果分类器的识别率很高，就是说分类器很容易识别出生成器生成的是假的样本，我们就人为生成器的性能不足，并且以分类器的结果作为生成器的loss，继续训练生成器，直到生成器生成的图像数据被判别器无法识别为是真样本还是假样本的地步为止。

* 这个比赛为什么会想到使用DCGAN？
因为GAN是典型的数据增强的方法。我们可以通过它来生成一些新的数据。这些数据具有原来的object有具有的特征，但是又和原来的图像不一样，这种不一样是和那种对原图通过形变，颜色变换，加噪声或者滤波之后的图像不一样。传统的方法都是基于原图进行修改，得到的其实是和原图差不多特征的数据。而GAN生成的是完全新的数据，并且这个数据和要表征的object很逼真，丰富了特征的多样性。这是一种更加高级的数据增强的方法。
最基本的GAN网络，它的网络内部是使用全连接层搭建的。DCGAN使用的是卷积层搭建的。实验证明DCGAN的效果比GAN的效果要好。为什么？
-》GAN可以用来拟合原始的数据分布，什么叫做拟合数据分布，就是给你一个训练数据，你能通过GAN这个工具，产生和这个数据分布相似的一些数据。







* 相关的问题：
  （1）pytorch中的反卷积的api函数的参数是怎样的，和卷积的函数有什么区别？

  （2）进行反卷积时候的上采样忘记了公式是怎样的？

  （3）conv2d以及ConvTranspose2d中，它的参数的顺序是in_channels, out_channels, kernel_size, 然后是strip以及padding，到底哪个先呢？为什么？

  -》 我觉得应该是strip先的，因为要进行上采样或者下采样，都是stride=2， 所以，然后再根据这个stride去计算padding的。
  （4）权重的初始化的方式有哪些？【今晚一定要仔细看看，然后自己测试一下，敲代码测试一下。30mins】

  （5）怎样指定不规则的kernel以及不规则的padding。在pytorch中，使用tupple（3， 5）。

  （6）leaky relu中的x小于0的那段的斜率一般怎样选？你的模型是选择的是多少？为什么这样选？
  -》我看到DCGAN这里是选择0.2的。

  （7）view和reshape的差异的比较。因为我看到一般都是使用view的，很少是使用reshape的。

  （8）

* 关键代码的解读

  ```python
  for epoch in range(train_epoch):
  	D_losses = []
      G_losses = []
      epoch_start_time = time.time()
      for x_, _ in train_loader:
          # here the x_ is the data, the _ is the lable, because we not need to use the lable, so we use the _ to ignore the label, we just need to use the image data
          # here set the claer all the D's grad to zero. that is very important. before you update your weight, you have to claer the gradient, so you have to call the functon that is the D.zero_grad()
          D.zero_grad()
          mini_batch = x_.size() # here to get the first element of the shape, that si the batch size in the pytorch.
          # and then we create the lable for the y_real and the y_fake
          y_real = torch.ones(mini_batch)
          # that is in the one epoch, we get the mini_batch nubmer of the real image, 
          # and we geneate the mini_batch number of the fake image;
          y_fake = torch.zeros(mini_batch)
          # then we get the result
          D_result = D(x_).squeeze 
          # x_ is the image data, and the D is the module, do this we can get the D_result, and the output is the probality, and we can tell the probality to make sure is the real or the fake smaple. That is the output of the classifier.
          # now we can get the loss of the modlue, after get the forward result, that is the loss. this is also the post deal.
          D_real_loss = BCE_loss(D_result, y_real_)
          # x_'s label is all the y_real, they are the same class, so we just asign value 1 to them. and we can get the BCE loss. And that is the real loss.
          z_ = torch.randn(mini_batch, 100).view(-1, 100, 1, 1)
          # now we generate the input of the G, that is the tensor: 1, 100, 1, 1: that is, the width and the height is hte one, and the channels is the 100, and that is just one data.
          G_result = G(z_)
          # now we put the z_ as the G's input, that we can get the G_result. and we can know that the z_ is the random tensor, and we can the rndom G_result.
          # we input the random tnesor: z_, that we can the random image, that is the G_result
          # until now, look at what we doing for D and the G:
          # for the D: we get the real image and the real lable x_, and we get the D's output, and we get the D_real loss. 
          # for the G: we input the random tensor, and we get tne random image.
          # see next what we will do!
          
          # the G_result is the random image that outputed by teh generator, and then we put the random image to the D, and then we can get the result. That is the D_result.
          D_result = D(G_result).suqeeze()
          D_fake_loss = BCE_losss(D_result, y_fake_)
          # because tha that the D_result is the fake image, so it's lable is the y_fake lable, so you can get the BCE_loss of them.
          # we can get the D_fake_score
          D_fake_score = D_result.data.mean()
          # now we can get the classifier's whole loss: 
          D_train_loss = D_real_loss + D_fake_loss
          # after getting the loss, we can backward to update the weight
          D_train_loss.backward()
          D_optimizer.step()
          # until now, we have traned the D discriminator one time.
          # and then we have to train the G generator
          G.zero_grad()
          z_ = troch.randn(mini_batch, 100).view(-1, 100, 1, 1)
          z_.cuda()
          # then we get the G_result
          G_result = G(z_)
          D_result = D(G_result).squeeze()
          G_train_loss = BCE_loss(D_result, y_real_)
          # here we can see the G's loss is the D_result's and the y_real
          G_train_loss.backward()
          G_optmizer.step()
          G_losses.append(G_train_loss.data[0])
          num_iter += 1
      epoch_end_time = time.time()
      per_epoch_ptim = epoch_end_time - epoch_start_time
      
  ```
  
  
